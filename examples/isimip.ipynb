{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import xarray as xr\n",
    "\n",
    "import climepi\n",
    "import climepi.climdata as climdata\n",
    "import climepi.epimod as epimod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_clim = climdata.get_example_dataset(\"isimip_london\")\n",
    "climepi_modes = ds_clim.climepi.modes\n",
    "ds_clim = (\n",
    "    ds_clim.drop_vars([\"lon_bnds\", \"lat_bnds\"])\n",
    "    .squeeze()\n",
    "    .sel({\"scenario\": [\"ssp126\", \"ssp370\", \"ssp585\"]})\n",
    ")\n",
    "ds_clim.climepi.modes = climepi_modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_clim_ym = ds_clim.climepi.yearly_average()\n",
    "ds_clim_ym.climepi.plot_time_series(\"temperature\", by=[\"scenario\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecol_niche_model = epimod.ecolniche.get_kaye_model()\n",
    "ds_clim.epimod.model = ecol_niche_model\n",
    "ds_epi = ds_clim.epimod.run_model()\n",
    "ds_months_suitable = ds_epi.epimod.months_suitable()\n",
    "ds_months_suitable.climepi.plot_time_series(by=[\"scenario\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ym = xr.merge([ds_clim_ym, ds_months_suitable])\n",
    "ds_ym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ym_stat = ds_ym.climepi.estimate_ensemble_stats()\n",
    "ds_ym_stat = ds_ym_stat[ds_ym_stat.climepi.get_non_bnd_data_vars()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_internal = ds_ym_stat.sel(ensemble_statistic=\"var\", drop=True).mean(\n",
    "    dim=[\"scenario\", \"model\"]\n",
    ")\n",
    "variance_internal.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_model = (\n",
    "    ds_ym_stat.sel(ensemble_statistic=\"mean\", drop=True)\n",
    "    .var(dim=\"model\")\n",
    "    .mean(dim=\"scenario\")\n",
    ")\n",
    "variance_model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_scenario = (\n",
    "    ds_ym_stat.sel(ensemble_statistic=\"mean\", drop=True)\n",
    "    .mean(dim=\"model\")\n",
    "    .var(dim=\"scenario\")\n",
    ")\n",
    "variance_scenario.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = ds_ym_stat.sel(ensemble_statistic=\"mean\").var(dim=[\"scenario\", \"model\"]) - (\n",
    "    variance_model + variance_scenario\n",
    ")\n",
    "check.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_variances = xr.concat(\n",
    "    [variance_internal, variance_scenario, variance_model],\n",
    "    dim=xr.Variable(\"variance_type\", [\"internal\", \"scenario\", \"model\"]),\n",
    "    coords=\"minimal\",\n",
    ")\n",
    "ds_variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_variance_area_plot(_ds_variance, var, proportions=False):\n",
    "    if proportions:\n",
    "        _ds_variance = _ds_variance / _ds_variance.sum(dim=\"variance_type\")\n",
    "    ds_new = xr.Dataset(\n",
    "        {\n",
    "            \"internal\": _ds_variance[var].sel(variance_type=\"internal\", drop=True),\n",
    "            \"scenario\": _ds_variance[var].sel(variance_type=\"scenario\", drop=True),\n",
    "            \"model\": _ds_variance[var].sel(variance_type=\"model\", drop=True),\n",
    "        }\n",
    "    )\n",
    "    return ds_new.squeeze().hvplot.area(\n",
    "        x=\"time\", y=[\"scenario\", \"model\", \"internal\"], group_label=\"Uncertainty type\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plume_plot(\n",
    "    _ds,\n",
    "    _ds_fit,\n",
    "    _ds_variances,\n",
    "    var,\n",
    "    conf_level=90,\n",
    "    scenario_baseline=None,\n",
    "    model_baseline=None,\n",
    "):\n",
    "    if scenario_baseline == \"mean\" and model_baseline == \"mean\":\n",
    "        da_baseline = _ds_fit[var].mean(dim=[\"model\", \"scenario\"])\n",
    "        da_fit_scenario_model_baseline = da_baseline\n",
    "        da_fit_scenario_baseline_model_mean = da_baseline\n",
    "    elif scenario_baseline == \"mean\":\n",
    "        raise ValueError(\n",
    "            \"If scenario_baseline is 'mean', model_baseline must be 'mean'\"\n",
    "        )\n",
    "    elif model_baseline == \"mean\":\n",
    "        da_baseline = _ds_fit[var].sel(scenario=scenario_baseline).mean(dim=\"model\")\n",
    "        da_fit_scenario_model_baseline = da_baseline\n",
    "        da_fit_scenario_baseline_model_mean = da_baseline\n",
    "    else:\n",
    "        da_baseline = _ds[var].sel(scenario=scenario_baseline, model=model_baseline)\n",
    "        da_fit_scenario_model_baseline = _ds_fit[var].sel(\n",
    "            scenario=scenario_baseline, model=model_baseline\n",
    "        )\n",
    "        da_fit_scenario_baseline_model_mean = (\n",
    "            _ds_fit[var].sel(scenario=scenario_baseline).mean(dim=\"model\")\n",
    "        )\n",
    "    da_variances = _ds_variances[var]\n",
    "    da_std_internal = np.sqrt(da_variances.sel(variance_type=\"internal\"))\n",
    "    if scenario_baseline == \"mean\":\n",
    "        da_std_internal_model = np.sqrt(\n",
    "            da_variances.sel(variance_type=[\"internal\", \"model\"]).sum(\n",
    "                dim=\"variance_type\"\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        da_std_internal_model = np.sqrt(\n",
    "            _ds_fit[var].sel(scenario=scenario_baseline).var(dim=\"model\")\n",
    "            + da_variances.sel(variance_type=\"internal\")\n",
    "        )\n",
    "    z = scipy.stats.norm.ppf(0.5 + conf_level / 200)\n",
    "    ds_internal = xr.Dataset(\n",
    "        {\n",
    "            \"low\": da_fit_scenario_model_baseline - z * da_std_internal,\n",
    "            \"high\": da_fit_scenario_model_baseline + z * da_std_internal,\n",
    "        }\n",
    "    )\n",
    "    ds_model = xr.Dataset(\n",
    "        {\n",
    "            \"low\": da_fit_scenario_baseline_model_mean - z * da_std_internal_model,\n",
    "            \"high\": da_fit_scenario_baseline_model_mean + z * da_std_internal_model,\n",
    "        }\n",
    "    )\n",
    "    da_fit_model_scenario_mean = _ds_fit[var].mean(dim=[\"model\", \"scenario\"])\n",
    "    da_std_internal_model_scenario = np.sqrt(da_variances.sum(dim=\"variance_type\"))\n",
    "    ds_scenario = xr.Dataset(\n",
    "        {\n",
    "            \"low\": da_fit_model_scenario_mean - z * da_std_internal_model_scenario,\n",
    "            \"high\": da_fit_model_scenario_mean + z * da_std_internal_model_scenario,\n",
    "        }\n",
    "    )\n",
    "    p_baseline = da_baseline.hvplot.line(color=\"k\", label=\"Baseline\")\n",
    "    p_internal = ds_internal.hvplot.area(x=\"time\", y=\"low\", y2=\"high\", label=\"Internal\")\n",
    "    p_model = ds_model.hvplot.area(x=\"time\", y=\"low\", y2=\"high\", label=\"Model\")\n",
    "    p_scenario = ds_scenario.hvplot.area(x=\"time\", y=\"low\", y2=\"high\", label=\"Scenario\")\n",
    "    return p_scenario * p_model * p_internal * p_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_var_plot = \"months_suitable\"\n",
    "scenario_baseline = \"mean\"\n",
    "model_baseline = \"mean\"\n",
    "# scenario_baseline = \"ssp370\"\n",
    "# model_baseline = \"gfdl-esm4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = ds_ym_stat.sel(ensemble_statistic=\"mean\", drop=True).climepi.plot_time_series(\n",
    "    data_var_plot, by=[\"scenario\"]\n",
    ")\n",
    "p2 = ds_ym.climepi.plot_time_series(data_var_plot, by=[\"scenario\"])\n",
    "p1 * p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_variance_area_plot(ds_variances, data_var_plot, proportions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_variance_area_plot(ds_variances, \"temperature\", proportions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ym.climepi.plot_var_decomp(data_var=data_var_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plume_plot(\n",
    "    ds_ym.squeeze(),\n",
    "    ds_ym_stat.sel(ensemble_statistic=\"mean\", drop=True).squeeze(),\n",
    "    ds_variances.squeeze(),\n",
    "    data_var_plot,\n",
    "    conf_level=90,\n",
    "    scenario_baseline=scenario_baseline,\n",
    "    model_baseline=model_baseline,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climepi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
